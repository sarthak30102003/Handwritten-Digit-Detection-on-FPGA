### ğŸ§  **Overall Architecture**

```text
Input Layer (784) 
    â†“
Hidden Layer 1 (128 neurons, sigmoid)
    â†“
Hidden Layer 2 (64 neurons, sigmoid)
    â†“
Output Layer (10 neurons, softmax)
```

---

### ğŸ” **Data Flow Through the Network**

#### 1. **Input Layer**

* Takes a flattened 28Ã—28 pixel grayscale image from the MNIST dataset â†’ 784 inputs.
* Scaled to values in the range \[0.01, 1.0] for better training convergence.

```python
params['A0'] = x_train  
```

#### 2. **Hidden Layer 1**

* Input: `A0` (784Ã—1)
* Weights: `W1` (128Ã—784)
* Weighted sum:
  `Z1 = W1 â€¢ A0`  â†’ shape: (128, 1)
* Activation:
  `A1 = sigmoid(Z1)`

#### 3. **Hidden Layer 2**

* Input: `A1` (128Ã—1)
* Weights: `W2` (64Ã—128)
* Weighted sum:
  `Z2 = W2 â€¢ A1`  â†’ shape: (64, 1)
* Activation:
  `A2 = sigmoid(Z2)`

#### 4. **Output Layer**

* Input: `A2` (64Ã—1)
* Weights: `W3` (10Ã—64)
* Weighted sum:
  `Z3 = W3 â€¢ A2`  â†’ shape: (10, 1)
* Activation:
  `A3 = softmax(Z3)`

  * Produces a one-hot encoded vector representing the predicted digit (0â€“9).

---

### ğŸ”„ **Backward Propagation (Training Phase)**

* Computes error at output â†’ propagates back through `W3`, `W2`, and `W1`.
* Uses derivatives of `sigmoid` and an **approximate derivative of softmax** (elemet-wise).
* `softmax` derivative is simplified here (not full Jacobian), but it's enough for basic gradient descent.

---

### ğŸ’¾ **Outputs and Monitoring**

* **Accuracy**: Calculated after each epoch using `compute_accuracy()`.
* **Weight Export**: You can export `.mem` and `.csv` files for hardware or documentation.
* **Layer-wise Excel Export**: Helpful for debugging or visualizing neuron behavior.

---

### ğŸ“Š Visualization Option (Simple)

```text
         [Input Image: 28x28]
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   A0 (784x1)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“ (W1)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Z1 â†’ A1 (128) â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“ (W2)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Z2 â†’ A2 (64)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“ (W3)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Z3 â†’ A3 (10)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```